---
title: 41｜统计学和大数据：为什么大多数企业用不好数据？
subtitle: 
author: 曲政
date: 2019-12-13
slug: 
tags:
- 
categories:
- Reference
typora-root-url: ../../static
show_toc: yes
---

你好，欢迎来到我的《数学通识 50 讲》。

我们上一讲讲到了估算概率通常使用的一种方法，就是依靠大数定理，根据统计得到概率。比如我们前面讲到，要了解一个汉语单词的概率，可以在很多语料中数一数它出现了多少次，然后将这个次数除以语料库中所有汉语词的次数即可。

只要统计量足够多，这个概率估计就是准确的。但是，什么算是足够多呢？

我们学习了方差的概念，就知道统计量应该达到某一个程度，使得方差足够小。正是因为有这一套理论支持，统计出来的结论才是可信的。我们今天很多企业使用数据决策，却失败了，就是因为所用的大数据，离开了这样的数学基础，得到的结论就不足以说服人。

今天我们就重点谈谈大数据方法的基础，也就是统计学，只有搞定其中的意义、方法和使用时的注意事项，在使用数据时，才算有了基本的行动指南。

## 用好大数据，你得先明白统计学

使用数据方法离不开统计学，那什么是统计学，它是否就是概率论的应用，是否是数学的一个分支？这些概念和关系很多人是一知半解。

统计学严格来讲是一门独立的科学，它是关于收集、分析、解释、陈述数据的科学。统计学的数学基础是概率论，在分析和解释数据时，要大量地使用概率论和其它数学工具，同时它也是概率论最大的用武之地，因此我们将它放在数学课中来讲。

但是，大家心里要清楚，统计学远不只是设计一个样本，然后用加减乘除算算概率那么单纯，它里面还有很多非数学的工作，比如如何陈述数据让大家接受你的结论，这也属于统计学的范畴。也正是为了这个目的，人们才发明了各种统计图表，因为人类对图表的敏感度要远远高于对数字的敏感度。

在统计学中，还专门有一个分支，叫做描述统计学，就是研究如何让统计的结果更有说服力。除此之外，统计学还有很多问题，比如如何保存和整理数据，其实也和数学没有太多的关系。

如果说概率论最初是赌徒们所研究的雕虫小技，登不上大雅之堂的话，那么统计学从一开始就是高大上的学问。统计学的英语单词 statistics 是源于拉丁语 “国会” 或者 “国民政治家” 的意思，最早是特指对国家的数据进行分析的学问。

18 世纪德国的学者戈特弗里德・阿亨瓦尔（Gottfried Achenwall）发明了德语版的这个词，特指 “研究国家的科学”，即根据数据了解情况，制定国策。后来这个词被翻译成各国的语言，但是含义却远远超出了原来特指研究国家的科学这一层含义。

统计学研究的目的，通常是从大量数据寻找规律性，不同因素之间的相关性，以及可能存在的因果关系。不过，后一种关系，即因果关系通常未必能找到，这一点我们后面要专门讲。在找到相应的规律之后，我们就可以利用它来建立数学模型，预估未来数据的发展和变化。

比如我们前面讲到，可以统计出汉语词之间的关联性，也就是条件概率，这样，如果遇到像 “天气” 和 “田七”，“北京” 和 “背景” 这样的同音近音词，我们就可以通过上下文，计算它们的条件概率，从而在语音识别，或者拼音输入中，确定到底是哪一个词。

比如，前面一个词是 “中药”，我们就知道后面是 “田七” 的可能性比 “天气” 大。而见到 “天气” 这个词，我们也就知道前面是 “北京” 比 “背景” 的可能性大。这就是统计的目的。

## 数据没用好的第一个原因：

近年来，由于数据量的剧增，一个企业要是不谈大数据都不好意思，但是你可能发现了，大数据谈了十年之后，也用了很多年，并非所有使用大数据的企业都在受益，很多企业使用它的效果不明显。这里面主要的原因是使用方法不对。

我们知道，今天使用大数据，主要是为了寻找一些变量之间的关联性，从而达到准确预测的目的。但是在实际问题中找准相关联的两个变量这件事本身并不容易。在前面讲到的利用前一个词预测后一个词，两个相关的变量就是前面的词和后面的词，当然也可以反过来。

今天我们知道可以这么使用之后，看似很容易想到，但是在语音识别诞生后的 20 多年里，科学家们并没有想到这个办法。因此虽然今天数据量不再是问题，但如何选定可能有关联的变量，则体现了人类的智慧。

特别是，当我们研究人类行为的时候，那些可能影响我们行为的客观变量或者说条件，更是不容易找到，即使找到，我们的行为又可能反过来改变条件。这里面最出名的例子就是上个世纪初，心理学家们在美国西屋电气公司位于霍桑市的工厂所进行的霍桑实验了。

霍桑实验的最初目的，是找到一些影响工人生产效率的因素（变量），然后加以改进，以提高生产率。心理学家们考虑的因素包括薪酬、照明条件、工间休息等等。

他们通过大量的统计发现，这些因素似乎和劳动效率有关，于是厂家就改善了相应的条件，比如增加照明亮度。但是，在这些改进中，一些因素并未达到对生产效率的明显提升，和想象的不一样，另一些改进虽然开始起到了一定的效果，但是很快又回到初始的状况。

对于这个现象，心理学家们后来进行了很多研究，比如发现当时很多实验并不是双盲的，那些对比在今天看来没有太多统计的意义，再比如当实验的设计者提高照明亮度开始测试生产效率时，工人似乎提高了效率，但这不是照明引起的，而是因为他们觉得自己被围观了，因此特别有干劲。

这一类的情况在早期的药品有效性的试验中也特别明显，只要病人从医生的口中觉察到他所服用的是真药而不是安慰剂，效果就好，但这无法判定是药的原因，还是心理作用。于是就有了 “霍桑效应” 这个名词，它是指当被观察者知道自己成为被观察对象而改变行为倾向的反应。

霍桑效应不仅体现在个人身上，也体现为群体的反应。比如一个国家将原本 3% 的 GDP 增长，按照 5% 公布于众，民众对经济前景有了信心，开始增加消费和扩大生产，反而可能导致 GDP 的上涨。

反过来，城市道路的拥堵信息一发布并显示在地图上之后，大家为了避免拥堵，都挤到地图上显示的绿色的道路中，反而造成了往哪里走，哪里就堵的死循环。此外，今天很多推荐系统见你读什么，买什么，就继续推荐什么，但你一点兴趣也没有，这就是陷入了霍桑效应的陷阱。

## 数据没用好的第二个原因：

今天大家在使用大数据时失效的另一个原因，就是低估了数据的稀疏性所带来的副作用。我们在前面讲了，利用统计得到结论，需要足够的统计量。今天看似大数据的数据量是足够的，但是如果你把它分为了很多维度，其实还是很稀疏的。

我们就以利用上下文预测后面的单词为例来说明，假如我们使用两个词 Y 和 Z 来预测第三个词 X，汉语的词汇量按照 10 万来计算，这看上去并不是一个复杂的数学模型，但是这个统计模型有 1000 万亿个条件概率值需要估算，整个互联网上的内容都翻译成中文，文字的总长度也超不过 100 万亿个词，因此，数据量显然是不够的。关于如何解决稀疏性的问题，我们明天会专门讲。

## 数据没用好的第三个原因：

大数据方法失效的第三个原因，就是把原因和结果搞反了。我们在前面介绍条件概率时讲到，X 和 Y 这两个随机变量，你既可以把 X 看成是 Y 的条件，也可以反过来看。当你拿到原始数据，看到 X 和 Y 同时出现时，你其实很难搞清楚谁是原因，谁是结果。

事实上很多研究人文社会科学的学者也经常把原因和结果搞反，因为你会同时读到把 X 当作 Y 原因的论著，以及把 Y 当作 X 原因的论著，它们甚至发表在同一本期刊上。

今天很多公司在使用大数据时，完全不去分析因果关系。比如我上网寻找过酒店，并非接下来就是要买飞机票，而是可能有一张用里程兑现的飞机票要到期了，必须用掉。从找酒店推断出要买机票就是搞反了因果关系。

## 想用好数据的五个步骤：

从这些例子可以看出，利用统计结果指导工作，远不像想象的那么容易。不过，使用统计的方法解决问题，通常还是有章可循的，我把它总结成下面五个步骤：

1. 设立研究目标，比如我们利用数据来证实什么假说，或者得到什么样的相关性。有了目标，才能够避免盲目使用数据的情况，并且能够有意识地过滤数据中的噪音。

通常，使用数据驱动的方法除了要准备一个待证实的假说，还要准备一个可对比的备用假说，比如你在证实药品有效性时，备用的假说就是安慰剂同样有效。统计的目的就是确认你的假说，同时否定掉备用假说。比如你要证明个人信息对推荐机票有效，就要证明不使用个人信息时，推荐机票无效，而不是同样有效。

2. 设计实验，选取数据。这些数据需要能够方便量化处理。比如你要识别图像，就需要将图像信息数字化，便于计算机处理。

3. 根据实验方案进行统计和实验，分析方差。很多人只是关注结果的均值，而忽略方差。比如你要想了解一种投资回报是否更高，光看回报率是不够的，还要衡量风险，就是方差。

4. 通过分析进一步了解数据，提出新假说。很多时候，统计的结果不是证明你的假说有效，而是证明它无效，这时就要提出新假说，重新验证。

5. 使用研究结果。这包括将你的统计结果用于产品，也包括报告给别人。对于后者来讲，怎么报告其实很有讲究。

要点总结：

我们介绍了统计学的来龙去脉，特别强调了它非数学的一面。然后我们从统计学的特点出发，讲了使用统计学的一些误区，特别是使用大数据方法的误区。最后介绍了使用统计学大致的方法。希望这些经验对你有所帮助。

欢迎大家在留言中提问，我在模块结束时会进行一次问答，期待你问出好问题。我们下一讲见。


### 用户留言


  - 日积月累 29 赞

    众所周知，当下最时髦的统计方法：大规模双盲随机对照实验。 过去，人们质疑上述实验的最大诟病就是样本量太小，由此产生的方差不能确保足够小。而要获取更大的数据样本，受技术、资金、时间、区域等限制，实现起来简直是比登天还难。 而现在，我们已进入了网络时代，数据规模出现了井喷，采集各类信息就变得唾手可得。可新问题也随之出现：民众的网上行为普遍更极端、更随性，这与他们在真实社会中的言行是大相径庭。 我想请教吴老师的是：如何有效涮选数据？

    2019 年 12 月 13 日

    作者回复我争取在问答环节里回答选取数据的问题。2019 年 12 月 13 日


  - Cynthia117 赞

    #数学助教# - 机器如何学习 1 （不慌，不会很难） 【机器在学什么】 简单理解，机器学习的是 “从 A 到 B 的映射关系（函数）F”。 # 没错，学的就是一个 “函数”～ 【关键词】 输入 A，输出 B，映射关系 F。 # 还记得我 25 讲笔记提到的魔盒吗？ 【举个栗子】 拿分类问题来说，给 AI 看一张图片，让它判断里面是猫还是狗。 这里的 “图片” 就是输入 A，“猫” 和 “狗” 就是输出 B。 机器学习就是通过各种统计手段，捣腾出一个 “魔盒” F，使得当我们放入猫的图片时，它能够正确地输出 “猫” 这个标签。 【输入与输出】 做任何事情，首先要明白我们最终想要什么。因为不同的目标 “输出”，决定了对 “输入” 的选择。 例如文本分类，输入一篇文章，我们的目标可以是 a）判断文章主题（经济学类，计算机类……） b）推测文章作者（例如鲁迅还是莎士比亚） c）识别它是否是一篇谣言 …… 如果目标是 a，那么在统计词频的时候，我们可能只需要文中的名词动词形容词，而要去掉停顿词和标点符号带来的 “噪声”； 如果目标是 b，那么虚词连词和停顿词，会对判断 “文风” 起到很大的作用； 如果目标是 c，那么我们绝不能忽视文中标点符号带来的信息量。 # 你的选择，取决于你的目标。 【模型复杂度】 明确了目标，我们需要选择这个 “魔盒” F 的 “基本框架”，也就是模型，那什么是 “模型” 呢？ （这里说的是 “狭义上” 的模型） 还是上面的文本分类的例子，回顾一下老师之前讲的方法 “文本向量化”，以及计算文本向量和 “余弦值”。（忘了的同学可以回顾 28 讲） 为了简单起见，假设所有文章都变成了一个 10 维的向量，并且只要区分 * 两类 * 文章（经济类，计算机类）。 1）F 的基本框架可以很简单。 我们可以再找一个 10 维的向量 a，并设定一个 “阈值” b，只要一篇文章的向量 x 与 a 的 “余弦值” 大于 b，就判断 x 是 “经济类” 文章，否则就是 “计算机类”。 可以说，我们用的这套判断方法，就是一个 “模型”，也就是 F 的基本框架。而这里用的，就是所谓的 “线性” 分类模型。 至于向量 a 和阈值 b，就是模型的 “参数”，我们事先不知道，但是可以通过样本 “训练” 出来。（如何训练就涉及了很多优化算法） 这个例子里，一共有 10+1=11 个参数需要我们训练学习。 2）F 可以再复杂一点。 我们可以先用 5 个 10 维的向量，分别与文章向量 x 计算余弦值，得到的 5 个余弦值，再分别与 5 个 “阈值” 比较，将小于阈值的那些设为 0，其它的余弦值保留它们本身。 上一步，我们将一个 10 维文本向量 x，变成了一个 5 维的向量。称为 “一层变换”。 接下来，可以再找一个 5 维向量和一个阈值，使用（1）中 “线性” 模型的方法，作出最后的判断。这一步称为 “输出层”。 如果用这样的模型，那么我们需要都少个参数呢？ 答案是第一层变换中的 (10+1) x5 和输出层的 (5+1) 之和，也就是 61 个参数。 # 这就是一个两层的 “神经网络”（或者说带有 * 一个隐含层 * 的神经网络），也就是 “深度学习” 的基础结构。 3）F 可以 “任意” 的复杂。 当然，我们可以进行 “多层变换”，只要重复多个 “一层变换”，最后再用一个 “输出层” 作判断。 由于我们可以重复任意多的 “层”，也就是说，模型可以任意复杂。 # 复杂模型的好处就是，可以相当精确，但是 “参数” 量也会剧增，“训练” 的难度也越来越大。 —————— 该如何选择一个复杂度合适的模型？ 如何 “训练” 一个模型？ —— 请听下回分解～

    2019 年 12 月 13 日


  - Allelujah 朱磊 45 赞

    对于老师在文中提到数据没用好的三个原因，我的理解是： 1. 通过数据得出的结论，无法将相关性推导为因果性； 2. 通过有限的数据，无法在多个因素间建立相关性； 3. 通过数据本身，无法确认相关性的方向。 也即是说，数据本身只能支持概率判断，无法给出逻辑解释。要想完成后者，还需要在前者的基础上做进一步的分析与操作，使得手中的数据能够尽可能降低信息熵。

    2019 年 12 月 13 日


  - 朱宇洲 上海 6 期 4 班 31 赞

    统计学不只是计算概率，如何收集、分析、解释、陈述数据都是统计学研究的范围。 整个统计分析过程中很重要的一环是找到因素间的关联关系，统计高手和一般人的核心区别，就是对关系洞察能力的强弱。

    2019 年 12 月 13 日


  - mengqi26 赞

    #数学助教# 统计学与大数据 统计学从大量数据寻找规律性、不同因素间的相关性，以及可能存在的因果关系，然后基于找到的规律建立数学模型，预估数据未来的发展变化。吴军老师在课程中讲到使用统计方法解决问题的五个步骤： 第一，设立研究目标 第二，设计实验，选取数据 第三，根据实验方案进行统计和实验，分析方差 第四，进一步分析数据，提出新假说，重新验证 第五，使用研究结果 这套方法其实也是科学探究的一般步骤，即 提出问题 —— 猜想与假设 —— 制定计划与设计实验 —— 进行实验与收集数据 —— 分析与论证 —— 评估结果 依据这个步骤，科研工作者规范化了从看到现象到解决问题的整个流程。在日常生活中，我们也可以利用这套方法解决问题，首先明确要解决的问题是什么，然后分析解决问题的关键是什么，接下来设计一套解决方案并实施，分析产生的效果，如果没有解决，进一步分析原因，设计新的解决方案。

    2019 年 12 月 13 日


  - 佛祖门徒 22 赞

    老师谈到企业使用大数据进行分析和决策的话题，让我感慨良多。公司一直都致力于帮助传统企业用好大数据，让决策更科学、更高效，并且在网络协同中逐渐实现数据智能决策。但是，大部分企业对大数据的概念比较热衷，而对正确应用知之甚少，如运用大数据究竟要实现哪些目标并不明确；以为各种数据越多越好；一旦数据分析结果与其管理经验不符，更愿意相信自己的直觉等。此外，很多大数据从业者并不善于包装和推广产品和分析结果，没有充分运用大数据可视化技术让用户直观体验到大数据分析能够发挥的重要作用。

    2019 年 12 月 13 日


  - 唐学长 19 赞

    今日得到：数据没用好的原因：使用方法不当 数据量稀疏 混淆因果关系 最后一个原因最容易避免，世界是相反的，凡事多问一句反过来呢？ 用好数据的五个步骤: 1，确立目标。做好控制组对照 2，设计方案。设计实验，选取数据 3，获取数据。根据实验方案进行统计和实验，分析方差。 4，深度挖掘。通过分析进一步了解数据，提出新假说。 5，应用研究结果。 于是就有了 “霍桑效应” 这个名词，它是指当被观察者知道自己成为被观察对象而改变行为倾向的反应。 人知道自己被关注之后的行为表现会异常。

    2019 年 12 月 13 日


  - 书画经纪人郭青峰 15 赞

    老师一下道出了大数据分析师存在的价值，5 个步骤就是数据分析应该的步骤，但大多数分析师不是这样工作的。 1 确立目标，本次分析的目的是什么 2 收集数据，本次分析的数据来源有哪些？能量化的全部要量化，要设计收集体系，并且日常化，达到有置信度的统计量；训练和测试、验证数据要一致，来自同一个分布，并且要检测。但同时还要避免幸存者偏差，数据要多纬度。不光公司内，可扩展到行业内，甚至市场横向对比。 3 训练数据，建立模型，这步相对简单，解决数据稀疏问题，可能需要做降维处理，稀疏矩阵分析，主成分分析。对业务数据要想当了解，或者直接和业务专家一起分析。数据是文本不可量化的，就要做独热矩阵转换或归一化、标准化，然后做去异常化。总之，大部分数据都是正态分布的，不是正态分布，需要转换成正态分布数据。没有相关性，不能硬做出相关性，更不能为了分析而分析，而忽略事实现象。 4 模型调优，调整模型参数，调整所用模型，使模型结果最优。结果实在不行，那就是数据有问题，如果你小步的微调没效果，对不起你只能从头再来，从收集数据开始。 5 数据分析报告要抓重点，不要把因果颠倒，本末倒置，要有逻辑，要做好 PPT，要会演讲.........

    2019 年 12 月 13 日


  - 戚志光 11 赞

    吴军老师在之前的专栏里讲过，大数据的时代要 “识数”，这个看似非常简单的要求，其实并不容易做到。 我以为互联网上的信息已经近乎无穷多了，用来做统计分析肯定够用，但是今天听吴军老师讲到了数据的稀疏性，才明白自己还是对数字的大和小缺乏认识。 从这个角度出发，也可以理解为什么现在最厉害的那些公司，还在拼命的搜集数据。因为在普通人看来多的不得了的数据，在数据科学家的眼里还是不够用的。一样东西只有稀缺才有价值，看似无穷的数据，其实还是稀缺的，所以才会有人需要它。

    2019 年 12 月 13 日


  - KID8 赞

    数据是资源还是只是一堆占硬盘空间的文件，取决于是否有人能从数据中获得信息，并依据这些信息做出更好的决策。 在曾鸣的《智能商业》一书中看到，淘宝一开始是收集每个商家的数据，制作成报告让商家自己做决策，但是发现他们都看不懂那些报告，有的甚至不看，这样的报告价值就很低了。 后来发现其实不必要让商家懂得这些报告，只要淘宝提供一个按钮，一按下去整个店铺的陈列展现就会被自动优化，从而带动销售额的提升。对于卖家来说，这样的操作再简单不过，只需要点击一下鼠标即可完成。 这其实就是淘宝的后台通过 “活数据” 的运营，对海量数据进行算法分析，最后智能化地帮助卖家自动优化店铺展现。

    2019 年 12 月 13 日


  - 陈 C7 赞

    今天学习的一些体会： 1. 概率论是统计学的核心工具，但统计学并不仅仅是概率，也不仅仅是数学，更多的时候，它需要逻辑、对人类认知的了解，等等。 2. 数据之间找到 “表面” 的关联并不难，但是很多看起来的相关性，其实是某些干扰因素的结果，或者只是我们真正关心的相关性的副作用，甚至有可能，这种关联只是一种巧合。 3. 为了分析、使用数据，指定维度是必要的步骤。更多的分析需求通常也意味着更丰富的维度，也就需要更加庞大的数据量。数据没有 “够用” 一说，而是往往 “多多益善” 的。 4. 虽然很多时候，大数据和人工智能的应用，并不真的需要明辨因果，但分清楚因果，对用好数据还是很有帮助的。找不到因果的相关性，看起来是一种规律，但拿来做干预和预测，却可能因为忽视甚至搞错了因果，而失去预期的效果。 5. 一个来自科研领域的思考：科学研究，需要的是给出假设，然后用数据去检验，但是如果反过来，通过研究数据关联来获得结论，那就要十分警惕了；这时候严谨的做法，应该是将观察相关性得到的假设，放到更多的实验统计中，进行更加细致的检验。 6. 一个来自生活经验的思考：生活中一些使用大数据的场景，需要的并不是严谨的逻辑，或者多么精准的预测和决策，而是通过发现数据中被我们忽视掉的关联，来为我们的生活提供更多的可能性。举一个我使用打车软件的例子：平常我都是在住处附近的公交车站打车，但是回到住处的时候，有时候会让司机多开一段距离，在离住处更近的地方下车。久而久之，打车软件就 “知道” 了我习惯下车的位置，导航目的地自动修改到了那边。类似的，网上购物的时候，我也曾经在平台的推荐中，意外地发现过，一些很贴合自己需求，但之前却并没有主动搜索过的商品。就如同 “林子大了什么鸟都有” 一样，数据多了（比如更多品类的商品），可能性也会变得更丰富。

    2019 年 12 月 13 日


  - Frank 林 7 赞

    吴军老师在《智能时代》一书讲到，『数据的处理和使用中，相关性是使用数据的钥匙，统计学是真正点石成金的魔棒。其中，数据模型是数据驱动方法的基础。』 数据驱动，是先有大量的数据，然后用很多简单的模型去契合数据。要成功，首先要具有统计学上的要求：第一数据量要足够大，第二样本要具有代表性。 数据驱动最大的优势在于，可以最大程度上得益于计算机技术的进步。

    2019 年 12 月 13 日


  - 李玳蒙 7 赞

    概率论与统计学的异同 1、相同点： 概率论与数理统计、统计学、应用统计学都是研究随机变量及其概率分布、数字特征、大数定律与中心极限定理、统计量及其概率分布、参数估计和假设检验、回归分析、方差分析、马尔科夫链等内容。 2、不同点： （1）概率论与数理统计属于数学的一个分支，它更注重于理论研究，它的结论广泛应用于各领域随机现象的研究。 概率论与数理统计的理论与方法已广泛应用于工业、农业、军事和科学技术中，如预测和滤波应用于空间技术和自动控制，时间序列分析应用于石油勘测和经济管理，马尔科夫过程与点过程统计分析应用于地震预测等 （2）社会统计学描述的是变量，数理统计学描述的是随机变量。 而变量和随机变量是两个既有区别又有联系，且在一定条件下可以相互转化的数学概念。社会统计学以变量为基础，数理统计学以随机变量为基矗。当变量取值的概率论与数理统计、统计学、应用统计学有什么相同。 3、统计学更注重应用，它的许多结论都来自于概率论与数理统计。数理统计更注重公式的推导，而统计学原理只是把数理统计的公式转换为更易用的形式。 扩展资料： 1、概率论与数理统计是数学的一个有特色且又十分活跃的分支，一方面，它有别开生面的研究课题，有自己独特的概念和方法，内容丰富，结果深刻；另一方面，它与其他学科又有紧密的联系，是近代数学的重要组成部分。 由于它近年来突飞猛进的发展与应用的广泛性，目前已发展成为一门独立的一级学科。 同时他又向基础学科、工科学科渗透，与其他学科相结合发展成为边缘学科，这是概率论与数理统计发展的一个新趋势。 2、统计学是通过搜索、整理、分析、描述数据等手段，以达到推断所测对象的本质，甚至预测对象未来的一门综合性科学。 统计学用到了大量的数学及其它学科的专业知识，其应用范围几乎覆盖了社会科学和自然科学的各个领域。 3、应用统计学系统讲述应用统计学基本知识和基本技能，融入电子表格的实际应用，介绍参数估计、假设检验等应用统计方法。

    2019 年 12 月 13 日


  - 新疆～林染 7 赞

    现在就足以明白，特立独行却正确是多么重要的了。希望我们都可以拥有足够的智慧，搜集到正确的数据，并能够合理的使用和阐述。

    2019 年 12 月 13 日


  - 费曼猫 7 赞

    希望吴军老师在多讲解下统计数据的稀疏性的概念 总结学习笔记 因果性和相关性并不同 霍桑效应 Hawthorne effect 是指当被观察者知道自己成为被观察对象而改变行为预期的反应。 individuals modify an aspect of their behavior in response to their awareness of being observed. 总结用好数据的步骤 1. 设计目标，附带假说 null hypothesis 和 alternative hypothesis 在确定假设的真假时，人们可能会犯两种错误： 一类错误是排除了真 null hypothesis 的错误（其概率记为 α）； 二类错误是没有排除掉假 null hypothesis 的错误（其概率记为 β）； 我们需要做的是显著性检验（Hypothesis Testing) 参考阅读：https://www.jianshu.com/p/8230223606e9 2. 选取数据，设计实验 3. 通过方差确定 risk 因为 variance 决定 volatility, and volatility is a measure of risk. 4. 分析数据设计更多的 Hypothesis 5. 使用研究成果 The work must serve real operational needs, we can't doing your own research in an ivory tower Analytical vigour, sense of curiosity and practical approach to solve problem not lament/describe it 是我们普通人接触和使用好统计学工具的不二法门，让他真正成为我们的 force multiplier

    2019 年 12 月 13 日


  - 燃简 4 赞

    之前花费半个月时间读了 朱迪亚珀尔 的 «the book of why» 因果 还是 关联？ 可以给这些 * 被动性的观察和相关性 *，* 人类行为主动干预 *,* 反事实的假设，人类幻想和行为 (过去 现在 未来)*, 这样三层 它可以解释和表述一些 概率 和统计模型的瓶颈。 或者说 概率和统计模型 工具，以及企图仅仅寻求 相关性，这个能力是有限的，很多现实问题不能解决。反而导致各种荒唐的结论 因为 1 吴军等老师都说过，在数学中的公式函数 概率等公式，基本上是双向的，即你可以把等式两边随意颠倒调换变量位置，意思是数学中的概率等，作为只解决相关性的，没有箭头，好像双向影响。 珀尔说： 可是现实中基本上不是这样。现实世界，基本上有箭头，x 可以导致 y, 但 y 不能导致 x. 比如气温升高，温度计上升。反过来，温度计上升可不能导致气温升高。他们没有可逆关系，要区分的。 因为 2 现实中 有存在那种被动性的自然而然的环境和人类行为，也存在那种人类主动的刻意的干预行为…… 比如医院是双盲随机试验那就是被动的自然行为。如果患者他知道实情，就不一样了。就是主动干预行为 森林里着火，是自然火，还是人为火 (可能是防火人员烧掉干柴以降低大火可能)，那么这样的就完全不一样。如果不加分别，都作为 “着火” 的数据，做概率计算和统计，会得出荒唐的结论。 这不是说人事先做变量分类就可以解决的，而是概率 和统计方法本身就不具备这个解决问题的能力，只会得出谬误。 因为 3, 有一些是反事实的想象，融入到现实中，其他的对于过去 现在未来的假设 。这是影响发展变化的超级重要因素。人脑会想到现实不存在的东西 (或者未曾发生的)，人脑会把符号想象为什么事物。才有了 社会系统 ，经济协作 和数学科学工程等 (他们都是符号系统，人脑想象他们就是现实中的什么东西。) 这是重要因素。得在模型工具里有。 概率数学和统计数学，寻求相关性的数学工具，根本就解决不了。就算是人为输入的时候合理设定了变量，也是白搭。是这两个学科本身作为工具，有局限性。 是数学这些学科本身局限，是学科本身发展不够，是数学的概率和统计 以及包括函数 线代等，他们本身还是有一些 * 粗糙 *。还是比较差劲。 是数学模型本身要有新学科 新表示法 新的思想。比 微积分 比概率论 统计学更超越的新数学。 不能说数学等本身就是 自娱自乐，或者自己体系的。不用对现实负责，这个格局一点不高。 数学要解决现实问题。 他说 观察到的 相关性的，只是因果认知的最基本层次，因果认知有三层。 他是贝叶斯网络的发明者，人工智能 统计领域合作专家。 他从 概率本位主义者 变为 因果 主义者 他是极有洞见的。 他有足够的 * 野心 *，创造全新的，和通达 真正智能

    2019 年 12 月 13 日


  - 山峰 4 赞

    今天的学习，发现了认知谬误，以往对于统计学，望文生义，认为统计学就是统合数据，进行计算的学科。理所当然的以为数学计算是重点，忽视了使用统计结果的人，尤其是人的需求（规律性、相关性、因果性）、内容（相对于数字，人对图表更敏感）、关系（上下级、朋友、同事……）。 带着好奇，我到网上搜索关于统计学的资料，加深对统计学的理解。 1、统计学的基本方式：根据小规模的代表性群体的信息，去猜测事物或数据背后的一般运行规律。 2、统计学与数学的区别，数学是一种演绎思想，统计学是一种归纳思想。 3、统计学过程实际上很大程度上是一个抽样过程，任何的统计研究都是如此。 4、统计学与经济学密切相关，统计学研究必须要尽量控制成本，用尽量小的成本来发现事物背后的真相，它是具有较好成本效益的一种方法学。 5、统计学分析的主要目标是发现真相，探索世界上事物运行的规律，常规的方法包括假设检验、回归分析两大类。作为统计学两大分析方法，差异性的假设检验方法和相关性的回归分析方法都从各自角度探讨变量与变量之间的关联性。 6、为实现样本的代表性，统计研究需要解决两个关键问题。第一，什么方式抽样才能得到代表性样本；第二，样本量是多大。

    2019 年 12 月 13 日


  - 奇・哈斯 3 赞

    数学通识课程笔记 #41 “大数据时代”，想用好数据并不容易，先得搞懂统计学。 1、认识统计学： —— 统计学（statistics）严格来讲是一门独立的科学，它是关于收集、分析、解释、陈述数据的科学。 —— 维基百科里的解释说，在资料分析的基础上，研究测定、收集、整理、归纳和分析反映数据资料，以便给出正确信息的科学。 —— 统计学研究的目的，通常是从大量数据寻找规律性，不同因素之间的相关性，以及可能存在的因果关系。 2、数据没有用好的原因： —— 今天数据量很大，但是找到其中有关联的变量则很难。比如，推荐系统根据一些关联的变量来推荐的内容中往往很多都是无用的。 —— 低估了数据的稀疏性所带来的副作用。比如，看似大数据的数据量是足够的，但是如果把它分为了很多维度，其实还是很稀疏的。 —— 把原因和结果搞反了。比如，拿到一堆原始数据，多种数据同时出现时，很难搞清楚谁是原因，谁是结果。 3、用好数据的五个步骤： ——1. 设立研究目标，比如，我们利用数据来证实什么假说，或者得到什么样的相关性。 ——2. 设计实验，选取数据。这些数据需要能够方便量化处理。比如，你要识别图像，就需要将图像信息数字化，便于计算机处理。 ——3. 根据实验方案进行统计和实验，分析方差。 ——4. 通过分析进一步了解数据，提出新假说。 ——5. 使用研究结果。包括将统计结果用于产品，也包括报告给别人。 再多的数据，如果没有用正确的方法把它用好，实现它的真正价值，那他就只会是 “大” 数据。

    2019 年 12 月 13 日


  - Lee Chen3 赞

    我觉得公司关心的应该是实际的使用效果，而不一定是准确度，比如： 1. 使用某算法，可以让单品或平台整体的推荐成功率从 10%，上升到 20%，那么就认为该算法是有效的。 2. 使用某算法，可以让单品或者平台整体的推荐成功率比竞争对手高 10%，那么就认为该算法是有效的。 3.. 如果满足前两点，却影响了部分用户的体验，那只能说十分抱歉，该问题留待今后优化了。甚至考虑到人和社会的复杂性是客观存在的，我想也很难做到对于每个推荐都 100% 准确，最终只能以整体数据作为判断依据。

    2019 年 12 月 13 日


  - Dx3 赞

    只能研究相关，不能研究因果，是概率论和统计学的基础架构决定的。（研究条件和结果的关系，但不涉及过程。即使讨论马尔可夫链，处理的也是基于条件的一系列结果。） 物理学中有一个概念叫 “光锥”，它可以用来描述因果到底是怎么一回事情。光锥以内存在因果，光锥以外没有因果关系。 但就人类目前的能力，是无法搜集这个维度的信息的，想从大数据中挖掘因果关系，暂时是没有理论和技术支撑的。 所以从一定程度上来说，除非有额外信息（例如吴军老师的里程积分，本次出行距离等），或者添加额外规则（购物后，不推荐同类商品）。否则光靠数据训练，只能获得相关性，就容易出现不合理推荐。 讲到这里，就可以回到之前加餐提到的数据资产化问题了，如果个人所有维度的信息在同一个数据体系下打通，各个服务都能访问到必要的数据，那么该用户就能获得更精准的服务。 在没有数据资产化的情况下，在同一平台服务中，个人让渡的隐私和信息越多，获得的服务也会越精准。 此外，这里还会涉及到老师在信息论中提过的，关于正交数据提升精准性的问题。 小企业即使专项信息量庞大，但维度单一，所以最终的误差就是大的，这个没有办法解决。 大平台的业务矩阵覆盖面大，拥有大量正交信息，即使专项信息比不过专业公司，但推荐的表现也可能远优于专业公司。 所以数据资产化，对于中小企业的赋能，会是空前的。

    2019 年 12 月 13 日

- 以上留言由 作者 筛选显示
